<!-- for modelscope yaml info
---
language:
- zh
tags:
- ancient-chat-llm
- internlm2
frameworks:
- pytorch
tasks:
- text-generation
license: Apache License 2.0
---
-->

# ancient-chat-llm å¤è¯­è¯´ â€”â€” ä¸€ä¸ªç²¾é€šä¸­å›½æ–‡åŒ–çš„å¤§æ¨¡å‹

<!-- PROJECT SHIELDS -->
<!-- 
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Stargazers][stars-shield]][stars-url]
-->

<br />
<!-- PROJECT LOGO -->

<p align="center">
  <a href="https://github.com/PeterH0323/ancient-chat-llm/">
    <img src="assets/logo.png" alt="Logo" width="30%">
  </a>

<h3 align="center">ancient-chat-llm</h3>
  <p align="center">
    <br />
    <a href="https://openxlab.org.cn/apps/detail/HinGwenWong/ancient-chat-llm">æŸ¥çœ‹Demo</a>
    Â·
    <a href="https://github.com/PeterH0323/ancient-chat-llm/issues">æŠ¥å‘ŠBug & æå‡ºæ–°ç‰¹æ€§</a>
  </p>
</p>

## ç®€ä»‹

**ancient-chat-llm å¤è¯­è¯´** æ˜¯ä¸€ä¸ªèƒ½å¤Ÿåœ¨ç”¨æˆ·è¾“å…¥ç°ä»£æ±‰è¯­åè¾“å‡ºæ–‡è¨€æ–‡ï¼ŒåŒæ—¶èƒ½å¤Ÿè§£ç­”ç”¨æˆ· **å…³äºä¸­å›½æ–‡åŒ–çš„é—®é¢˜** çš„å¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº**å”è¯—ã€å®‹è¯ã€è®ºè¯­**ç­‰å¤ç±ï¼Œè¿˜å¯ä»¥è®©å…¶**å°†æ–‡è¨€æ–‡ç¿»è¯‘æˆç™½è¯æ–‡**ç­‰ï¼Œæ¨¡å‹ç”¨ [xtuner](https://github.com/InternLM/xtuner) åœ¨ [InternLM2](https://github.com/InternLM/InternLM) çš„åŸºç¡€ä¸ŠæŒ‡ä»¤å¾®è°ƒè€Œæ¥ã€‚

**å¼€æºä¸æ˜“ï¼Œå¦‚æœæœ¬é¡¹ç›®å¸®åˆ°å¤§å®¶ï¼Œå¯ä»¥å³ä¸Šè§’å¸®æˆ‘ç‚¹ä¸ª star~ â­â­ , æ‚¨çš„ star â­æ˜¯æˆ‘ä»¬æœ€å¤§çš„é¼“åŠ±ï¼Œè°¢è°¢å„ä½ï¼**  

## NEWS

- [2024.2.3]  æ•°æ®æ¸…æ´—ï¼Œå‘å¸ƒè¿­ä»£æ¨¡å‹
- [2024.1.28] æ–°å¢è¯—è¯ã€å¤ç±ç­‰çŸ¥è¯†å¾®è°ƒæ¨¡å‹
- [2024.1.16] æˆè¯­æ•°æ®é›†å¾®è°ƒæ¨¡å‹

## ä»‹ç»

ä¸­å›½æ–‡åŒ–ï¼Œåšå¤§ç²¾æ·±ï¼Œæºè¿œæµé•¿ã€‚ä»å¤è€çš„è¯—è¯æ­Œèµ‹åˆ°ç°ä»£çš„æ–‡è‰ºåˆ›ä½œï¼Œéƒ½å±•ç°äº†ä¸­åæ°‘æ—çš„æ™ºæ…§å’Œåˆ›é€ åŠ›ã€‚

- **ä¸­å›½å¤ç±**ï¼Œä¸­åæ–‡æ˜çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ‰¿è½½ç€ä¸°å¯Œçš„å†å²å’Œæ–‡åŒ–ä¿¡æ¯ï¼Œåæ˜ äº†å¤ä»£ç¤¾ä¼šçš„é£è²Œå’Œäººæ°‘çš„æ™ºæ…§ã€‚è¿™äº›å¤ç±ä¸ä»…å…·æœ‰æé«˜çš„å†å²ä»·å€¼ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬äº†è§£å¤ä»£æ–‡åŒ–ã€ä¼ æ‰¿ä¸­åæ–‡æ˜çš„é‡è¦çª—å£ã€‚å…¶ä¸­ï¼Œã€Šè¯—ç»ã€‹æ˜¯ä¸­å›½æœ€æ—©çš„è¯—æ­Œæ€»é›†ï¼Œæ”¶å½•äº†è¥¿å‘¨åˆå¹´è‡³æ˜¥ç§‹ä¸­å¶çš„è¯—æ­Œï¼Œå±•ç°äº†å¤ä»£äººæ°‘çš„ç”Ÿæ´»å’Œæƒ…æ„Ÿã€‚å…¶ä¼˜ç¾çš„è¯­è¨€å’Œæ·±é‚ƒçš„æ€æƒ³ï¼Œè‡³ä»Šä»ä¸ºäººä»¬æ‰€ä¼ é¢‚å’Œå­¦ä¹ ã€‚å¦ä¸€éƒ¨é‡è¦çš„å¤ç±æ˜¯ **ã€Šè®ºè¯­ã€‹**ï¼Œå…¶æ˜¯å„’å®¶å­¦æ´¾çš„ç»å…¸ä¹‹ä½œï¼Œè®°å½•äº†å­”å­åŠå…¶å¼Ÿå­çš„è¨€è¡Œå’Œæ€æƒ³ã€‚å®ƒå¼ºè°ƒä»çˆ±ã€ç¤¼ä¹‰ç­‰å„’å®¶æ ¸å¿ƒä»·å€¼è§‚ï¼Œå¯¹ä¸­å›½ä¹ƒè‡³ä¸œäºšåœ°åŒºçš„æ–‡åŒ–å’Œç¤¾ä¼šäº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚æ­¤å¤–ï¼Œ**ã€Šé“å¾·ç»ã€‹ã€ã€Šæ˜“ç»ã€‹** ç­‰é“å®¶ç»å…¸ï¼Œä»¥åŠ **ã€Šå­™å­å…µæ³•ã€‹ã€ã€Šæˆ˜å›½ç­–ã€‹** ç­‰å…µå®¶è‘—ä½œï¼Œä¹Ÿéƒ½æ˜¯ä¸­å›½å¤ä»£æ–‡åŒ–å¤ç±ä¸­çš„é‡è¦ä»£è¡¨ã€‚

- **ä¸­å›½å¤è¯—**ï¼Œè•´å«ç€æ·±åšçš„æ–‡åŒ–åº•è•´ï¼Œé—ªè€€ç€è¯—äººçš„æ™ºæ…§ä¸æ‰æƒ…ã€‚ä»¥æç™½çš„ **ã€Šå°†è¿›é…’ã€‹** ä¸ºä¾‹ï¼Œè¯—ä¸­â€œäººç”Ÿå¾—æ„é¡»å°½æ¬¢ï¼Œè«ä½¿é‡‘æ¨½ç©ºå¯¹æœˆâ€ä¼ è¾¾å‡ºè±è¾¾ä¹è§‚çš„äººç”Ÿæ€åº¦ï¼Œæ¿€åŠ±ç€ä»£ä»£è¯»è€…ã€‚è¿™æ ·çš„è¯—å¥ï¼Œæ—¢æ˜¯ä¸­å›½å¤è¯—çš„ç‘°å®ï¼Œä¹Ÿæ˜¯ä¸­åæ–‡åŒ–çš„éª„å‚²ã€‚è®©æˆ‘ä»¬å…±åŒæ¬£èµã€ä¼ æ‰¿è¿™äº›çè´µçš„æ–‡åŒ–é—äº§ï¼Œæ„Ÿå—ä¸­å›½å¤è¯—çš„æ— ç©·é­…åŠ›ã€‚

- **ä¸­å›½æˆè¯­**ï¼Œå…¶æœ‰å›ºå®šçš„ç»“æ„å½¢å¼å’Œå›ºå®šçš„è¯´æ³•ï¼Œè¡¨ç¤ºä¸€å®šçš„æ„ä¹‰ï¼Œåœ¨è¯­å¥ä¸­æ˜¯ä½œä¸ºä¸€ä¸ªæ•´ä½“æ¥åº”ç”¨çš„ã€‚æˆè¯­æœ‰å¾ˆå¤§ä¸€éƒ¨åˆ†æ˜¯ä»å¤ä»£ç›¸æ‰¿æ²¿ç”¨ä¸‹æ¥çš„ï¼Œå®ƒä»£è¡¨äº†ä¸€ä¸ªæ•…äº‹æˆ–è€…å…¸æ•…ï¼Œæœ‰äº›æˆè¯­æœ¬å°±æ˜¯ä¸€ä¸ªå¾®å‹çš„å¥å­ã€‚æœ‰äº›æˆè¯­æ¥è‡ªäºå†å²äº‹ä»¶ï¼Œå¦‚â€œå®Œç’§å½’èµµâ€ã€â€œè´Ÿè†è¯·ç½ªâ€ç­‰ï¼Œå®ƒä»¬é€šè¿‡ç®€çŸ­çš„å½¢å¼ï¼Œæ¦‚æ‹¬äº†æ•´ä¸ªæ•…äº‹çš„å†…å®¹ï¼Œä½¿å¾—äººä»¬å¯ä»¥æ›´åŠ æ–¹ä¾¿åœ°ç†è§£å’Œè®°å¿†ã€‚æœ‰äº›æˆè¯­åˆ™æ¥è‡ªäºæ–‡å­¦ä½œå“ï¼Œå¦‚â€œæŸ³æš—èŠ±æ˜â€ã€â€œåˆ»èˆŸæ±‚å‰‘â€ç­‰ï¼Œè¿™äº›æˆè¯­é€šè¿‡å½¢è±¡çš„æ¯”å–»ï¼Œè¡¨è¾¾äº†æ·±åˆ»çš„é“ç†ã€‚

**è¿™å°±æ˜¯æˆ‘ä»¬åšè¿™ä¸ªæ¨¡å‹çš„åˆè¡·ï¼Œæˆ‘ä»¬æƒ³å°†ä¸­åæ–‡åŒ–æ•™ç»™å¤§æ¨¡å‹ï¼Œè®©å…¶èƒ½å¤Ÿå°½å¯èƒ½æŒæ¡ä¸­åæ–‡åŒ–ï¼Œåšåˆ°æ–‡åŒ–è¾“å‡ºã€‚**

**ancient-chat-llm å¤è¯­è¯´** æ˜¯ä¸€ä¸ªèƒ½å¤Ÿåœ¨ç”¨æˆ·è¾“å…¥ç°ä»£æ±‰è¯­åè¾“å‡ºæ–‡è¨€æ–‡ï¼ŒåŒæ—¶èƒ½å¤Ÿè§£ç­”ç”¨æˆ· **å…³äºä¸­å›½æ–‡åŒ–çš„é—®é¢˜** çš„å¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº**å”è¯—ã€å®‹è¯ã€è®ºè¯­**ç­‰å¤ç±ï¼Œè¿˜å¯ä»¥è®©å…¶**å°†æ–‡è¨€æ–‡ç¿»è¯‘æˆç™½è¯æ–‡**ç­‰ï¼Œæ¨¡å‹ç”¨ [xtuner](https://github.com/InternLM/xtuner) åœ¨ [InternLM2](https://github.com/InternLM/InternLM) çš„åŸºç¡€ä¸ŠæŒ‡ä»¤å¾®è°ƒè€Œæ¥ã€‚

**å¼€æºä¸æ˜“ï¼Œå¦‚æœæœ¬é¡¹ç›®å¸®åˆ°å¤§å®¶ï¼Œå¯ä»¥å³ä¸Šè§’å¸®æˆ‘ç‚¹ä¸ª star~ â­â­ , æ‚¨çš„ star â­æ˜¯æˆ‘ä»¬æœ€å¤§çš„é¼“åŠ±ï¼Œè°¢è°¢å„ä½ï¼**  


## æ¼”ç¤º

Demo è®¿é—®åœ°å€ï¼šhttps://openxlab.org.cn/apps/detail/HinGwenWong/ancient-chat-llm

<p align="center">
    <img src="assets/demo.png" alt="Demo" width="70%">
</p>

æ¨¡å‹å¯¹æ¯”ï¼šcomming soon


## Model Zoo

| æ¨¡å‹                | åŸºåº§             | æ•°æ®é‡                   | ModelScope(HF)                                                          | Transformers(HF)                                               | OpenXLab(HF)                                                                                                                                                |
| ------------------- | ---------------- | ------------------------ | ----------------------------------------------------------------------- | -------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ancient-chat-llm-7b | interlm2-chat-7b | 230013 ä¸ªå• conversation | [ModelScope](https://modelscope.cn/models/HinGwenWoong/ancient-chat-7b) | [hugging face](https://huggingface.co/hingwen/ancient-chat-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/HinGwenWong/ancient-chat-llm-7b) |


<details>
<summary> ä» ModelScope å¯¼å…¥</summary>

```python
import torch
from modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM
model_dir = snapshot_download('HinGwenWoong/ancient-chat-7b')
tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map="auto", trust_remote_code=True)
# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and might cause OOM Error.
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, torch_dtype=torch.float16)
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
response, history = model.chat(tokenizer, "æç™½ç®€ä»‹", history=history)
print(response)
```

</details>

<details>
<summary> ä» huggingface å¯¼å…¥ </summary>

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("hingwen/ancient-chat-7b", trust_remote_code=True)
# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and might cause OOM Error.
model = AutoModelForCausalLM.from_pretrained("hingwen/ancient-chat-7b", device_map="auto", trust_remote_code=True, torch_dtype=torch.float16)
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
response, history = model.chat(tokenizer, "æç™½ç®€ä»‹", history=history)
print(response)
```

</details>

## çŸ¥è¯†åº“

- [x] æ–‡è¨€æ–‡ç¿»è¯‘
- [x] æˆè¯­
- [x] è®ºè¯­
- [x] å”è¯—
- [x] å®‹è¯
- [x] æ¥šè¾
- [x] å››ä¹¦äº”ç»
- [x] ç™¾å®¶å§“
- [x] å¼Ÿå­è§„
- [ ] å²è®°
- [ ] å®«å»·åˆ¶åº¦
- [ ] äºŒåå››èŠ‚æ°”
- [ ] ...

## ç¯å¢ƒæ­å»º

æœ¬é¡¹ç›®ä½¿ç”¨ [xtuner](https://github.com/InternLM/xtuner) è®­ç»ƒï¼Œåœ¨ [internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b) ä¸Šè¿›è¡Œå¾®è°ƒ

1. clone æœ¬é¡¹ç›®

```bash
git clone https://github.com/PeterH0323/ancient-chat-llm.git
cd ancient-chat-llm
```

2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
conda env create -f environment.yml
conda activate ancient-chat-llm
pip install -r requirements-raw.txt
```

## æ•°æ®é›†å‡†å¤‡

ç›®å‰ä½¿ç”¨åˆ°çš„å¼€æºæ•°æ®é›†æœ‰ä»¥ä¸‹å‡ ä¸ªï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨çˆ¬è™«ç­‰æŠ€æœ¯è¿›è¡Œçˆ¬å–äº†å…¶ä½™çŸ¥è¯†åº“çš„æ•°æ®é›†ï¼š

- æ–‡è¨€æ–‡ï¼šhttps://huggingface.co/datasets/RUCAIBox/Erya-dataset/tree/main
- å¤è¯—ï¼šhttps://github.com/chinese-poetry/chinese-poetry

æ•°æ®é›†ç»“æ„ï¼ˆçœç•¥äº†ç”¨ä¸åˆ°çš„æ–‡ä»¶ï¼‰ï¼š

```bash
dataset/
â”œâ”€â”€ Erya-dataset
â”‚   â”œâ”€â”€ dataset # è§£å‹è‡ª finetune.tgz
â”‚   â””â”€â”€ stage_2 # è§£å‹è‡ª trans.tgz 
â”œâ”€â”€ chinese-poetry
â”‚   â”œâ”€â”€ äº”ä»£è¯—è¯
â”‚   â”œâ”€â”€ å…ƒæ›²
â”‚   â”œâ”€â”€ å…¨å”è¯—
â”‚   â”œâ”€â”€ å››ä¹¦äº”ç»
â”‚   â”œâ”€â”€ å®‹è¯
â”‚   â”œâ”€â”€ å¹½æ¢¦å½±
â”‚   â”œâ”€â”€ å¾¡å®šå…¨å”è©©
â”‚   â”œâ”€â”€ æ›¹æ“è¯—é›†
â”‚   â”œâ”€â”€ æ¥šè¾
â”‚   â”œâ”€â”€ æ°´å¢¨å”è¯—
â”‚   â”œâ”€â”€ çº³å…°æ€§å¾·
â”‚   â”œâ”€â”€ è’™å­¦
â”‚   â”œâ”€â”€ è®ºè¯­
â”‚   â””â”€â”€ è¯—ç»
â”œâ”€â”€ gen_dataset.py
â””â”€â”€ whoami.jsonl
```

ä½¿ç”¨è„šæœ¬å¯ä»¥è¿›è¡Œç”Ÿæˆï¼š

```bash
cd dataset
python gen_dataset.py --data-root=./dataset --output=data.jsonl
```

ç”Ÿæˆä¹‹åå¯ä»¥åœ¨ ${output} è®¾ç½®çš„è·¯å¾„çœ‹åˆ° jsonl æ–‡ä»¶ï¼Œæ ·ä¾‹å¦‚ä¸‹ï¼š

```json
    {
        "conversation": [
            {
                "system": "ä½ ç²¾é€šä¸­å›½æ–‡åŒ–å’Œä¸­æ–‡çŸ¥è¯†ï¼Œä½ æ€»èƒ½è§£ç­”ç”¨æˆ·å…³äºä¸­å›½æ–‡åŒ–å’Œä¸­æ–‡çš„ç›¸å…³çŸ¥è¯†ã€‚",
                "input": "å¸®æˆ‘ç¿»è¯‘æˆæ–‡è¨€æ–‡ï¼šâ€œå¾€è¥¿èµ°ä¸€é‡Œï¼Œç»•ç€æ–°å¯ºè¥¿è¾¹çš„å³°å˜´å¾€åŒ—èµ°ã€‚â€",
                "output": "è¥¿ä¸€é‡Œï¼Œè½¬æ–°å¯ºè¥¿å³°ä¹‹å˜´è€ŒåŒ—ã€‚"
            }
        ]
    },
    {
        "conversation": [
            {
                "system": "ä½ ç²¾é€šä¸­å›½æ–‡åŒ–å’Œä¸­æ–‡çŸ¥è¯†ï¼Œä½ æ€»èƒ½è§£ç­”ç”¨æˆ·å…³äºä¸­å›½æ–‡åŒ–å’Œä¸­æ–‡çš„ç›¸å…³çŸ¥è¯†ã€‚",
                "input": "èƒŒè¯µã€Šå°†è¿›é…’ã€‹",
                "output": "å›ä¸è§é»„æ²³ä¹‹æ°´å¤©ä¸Šæ¥ï¼Œå¥”æµåˆ°æµ·ä¸å¤å›ã€‚\nå›ä¸è¦‹é«˜å ‚æ˜é¡æ‚²ç™½é«®ï¼Œæœå¦‚é’çµ²æš®æˆé›ªã€‚\näººç”Ÿå¾—æ„é ˆç›¡æ­¡ï¼Œè«ä½¿é‡‘æ¨½ç©ºå°æœˆã€‚\nå¤©ç”Ÿæˆ‘æå¿…æœ‰ç”¨ï¼Œåƒé‡‘æ•£ç›¡é‚„å¾©ä¾†ã€‚\nçƒ¹ç¾Šå®°ç‰›ä¸”çˆ²æ¨‚ï¼Œæœƒé ˆä¸€é£²ä¸‰ç™¾ç›ƒã€‚\nå²‘å¤«å­ï¼Œä¸¹ä¸˜ç”Ÿï¼Œå°‡é€²é…’ï¼Œå›è«åœã€‚\nèˆ‡å›æ­Œä¸€æ›²ï¼Œè«‹å›çˆ²æˆ‘å´è€³è½ã€‚\né˜é¼“é¥Œç‰ä¸è¶³è²´ï¼Œä½†é¡˜é•·é†‰ä¸é¡˜é†’ã€‚\nå¤ä¾†è–è³¢çš†å¯‚å¯ï¼ŒæƒŸæœ‰é£²è€…ç•™å…¶åã€‚\né™³ç‹æ˜”æ™‚å®´å¹³æ¨‚ï¼Œæ–—é…’ååƒæ£è®™è¬”ã€‚\nä¸»äººä½•çˆ²è¨€å°‘éŒ¢ï¼Œå¾‘é ˆæ²½å–å°å›é…Œã€‚\näº”èŠ±é¦¬ï¼Œåƒé‡‘è£˜ï¼Œå‘¼å…’å°‡å‡ºæ›ç¾é…’ï¼Œèˆ‡çˆ¾åŒéŠ·è¬å¤æ„ã€‚"
            }
        ]
    },
    ...
```



## è®­ç»ƒ

1. è®­ç»ƒä¹‹å‰ï¼Œéœ€è¦åœ¨ `xtuner` ä»£ç ä¸­ `xtuner/xtuner/utils/templates.py` æ·»åŠ  `SYSTEM_TEMPLATE.ancient_chat` ï¼š

```diff
SYSTEM_TEMPLATE = ConfigDict(
    moss_sft=('You are an AI assistant whose name is {bot_name}.\n'
              'Capabilities and tools that {bot_name} can possess.\n'
              '- Inner thoughts: enabled.\n'
              '- Web search: enabled. API: Search(query)\n'
              '- Calculator: enabled. API: Calculate(expression)\n'
              '- Equation solver: enabled. API: Solve(equation)\n'
              '- Text-to-image: disabled.\n'
              '- Image edition: disabled.\n'
              '- Text-to-speech: disabled.\n'),
    alpaca=('Below is an instruction that describes a task. '
            'Write a response that appropriately completes the request.\n'),
    arxiv_gentile=('If you are an expert in writing papers, please generate '
                   "a good paper title for this paper based on other authors' "
                   'descriptions of their abstracts.\n'),
    colorist=('You are a professional color designer. Please provide the '
              'corresponding colors based on the description of Human.\n'),
    coder=('You are a professional programer. Please provide the '
           'corresponding code based on the description of Human.\n'),
    lawyer='ä½ ç°åœ¨æ˜¯ä¸€åä¸“ä¸šçš„ä¸­å›½å¾‹å¸ˆï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå‡†ç¡®ã€æœ‰ç†æœ‰æ®çš„å›å¤ã€‚\n',
    medical='å¦‚æœä½ æ˜¯ä¸€ååŒ»ç”Ÿï¼Œè¯·æ ¹æ®æ‚£è€…çš„æè¿°å›ç­”åŒ»å­¦é—®é¢˜ã€‚\n',
    sql=('If you are an expert in SQL, please generate a good SQL Query '
         'for Question based on the CREATE TABLE statement.\n'),``````
+    ancient_chat="ä½ ç²¾é€šä¸­å›½æ–‡åŒ–å’Œä¸­æ–‡çŸ¥è¯†ï¼Œä½ æ€»èƒ½è§£ç­”ç”¨æˆ·å…³äºä¸­å›½æ–‡åŒ–å’Œä¸­æ–‡çš„ç›¸å…³çŸ¥è¯†ã€‚\n",
)
```

2. å°† `./finetune_configs/internlm2_chat_7b/internlm2_chat_7b_qlora_custom_data_finetune.py` ä¸­ æ•°æ®é›†è·¯å¾„ å’Œ æ¨¡å‹è·¯å¾„ æ”¹ä¸ºæ‚¨çš„æœ¬åœ°è·¯å¾„

```diff
# Model
- pretrained_model_name_or_path = 'internlm/internlm2-7b'
+ pretrained_model_name_or_path = '/path/to/internlm/internlm2-7b' # è¿™æ­¥å¯é€‰ï¼Œå¦‚æœäº‹å…ˆä¸‹è½½å¥½äº†æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨ç»å¯¹è·¯å¾„

# Data
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = '/path/to/data.jsonl' # æ•°æ®é›†æ­¥éª¤ç”Ÿæˆçš„ json æ–‡ä»¶ç»å¯¹è·¯å¾„
prompt_template = PROMPT_TEMPLATE.default
max_length = 2048
pack_to_max_length = True
```

3. ä½¿ç”¨å‘½ä»¤è¿›è¡Œè®­ç»ƒï¼š

```bash
xtuner train finetune_configs/internlm2_chat_7b/internlm2_chat_7b_qlora_custom_data_finetune.py --deepspeed deepspeed_zero2
```

æ³¨æ„ï¼šå¦‚æœæ˜¾å­˜ä¸å¤Ÿäº†ï¼Œè°ƒå°ä¸€ç‚¹ `batch_size` å’Œ `max_length`ï¼Œåä¹‹è¿˜å‰©å¾ˆå¤šï¼Œè°ƒå¤§è¿™ä¸¤ä¸ªå€¼

## éƒ¨ç½²

### Web éƒ¨ç½² Demo

1. å°† pth è½¬ä¸º hf 

```bash
xtuner convert pth_to_hf ./finetune_configs/internlm_chat_7b/internlm2_chat_7b_qlora_custom_data_finetune.py \
                         ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10.pth \
                         ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10_hf
```

2. å°†å¾®è°ƒåçš„æ¨¡å‹å’Œæºæ¨¡å‹ merge ç”Ÿæˆæ–°çš„æ¨¡å‹

```bash
export MKL_SERVICE_FORCE_INTEL=1 # è§£å†³ Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.
xtuner convert merge /path/to/internlm2-chat-7b \
                     ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10_hf \
                     ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10_merge
```

3. å¯åŠ¨ web demo

```bash
streamlit run web_demo.py --server.address=0.0.0.0 --server.port 7860
```

<!-- # ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œ cli çš„æ–¹å¼è¿›è¡Œå¯åŠ¨
xtuner chat ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10_merge \
            --prompt-template internlm2_chat \
            --system-template ancient_chat -->

### LMDeploy 

1. å®‰è£… lmdeploy

```bash
pip install 'lmdeploy[all]==v0.2.1'
```

1. è¿›è¡Œ 4bit é‡åŒ–

```bash
lmdeploy lite auto_awq ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10_merge \
                       --calib-dataset 'c4' \
                       --calib-samples 128 \
                       --calib-seqlen 2048 \
                       --w-bits 4 \
                       --w-group-size 128 \
                       --work-dir ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10_merge-4bit

```

## æ¨¡å‹æµ‹è¯„

ä½¿ç”¨çš„æ¨¡å‹æµ‹è¯„æ¡†æ¶ä¸º [opencompass](https://github.com/open-compass/opencompass)

1. æ­å»ºç¯å¢ƒ

```bash
git clone https://github.com/open-compass/opencompass
cd opencompass
pip install -e .
export PYTHONPATH=$(pwd)
```

2. å¯åŠ¨æµ‹è¯„

- CEval

```bash
python run.py --datasets ceval_gen \
              --hf-path ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10_merge/ \
              --tokenizer-path ./work_dirs/internlm2_chat_7b_qlora_custom_data_finetune/epoch_10_merge / \
              --tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True \
              --model-kwargs trust_remote_code=True device_map='auto' \
              --max-seq-len 2048 \
              --max-out-len 16 \
              --batch-size 4 \
              --num-gpus 1 \
              --debug
```

æµ‹è¯„ç»“æœï¼š[ceval_gen](eval_report/ceval_gen)

## TODO

- [x] é‡åŒ–æ¨¡å‹
- [x] æ¨¡å‹ä»éœ€è¿­ä»£
- [ ] æ•°æ®é›†éœ€è¦æ¸…æ´—
- [ ] ä½¿ç”¨å…¶å®ƒå¤§æ¨¡å‹è¿›è¡Œæ•°æ®é›†æ‰©å……

## åè®°

æœ¬é¡¹ç›®å±äºä¸ªäººçš„ä¸€ä¸ªå­¦ä¹ é¡¹ç›®ï¼Œè¿˜æœ‰å¾ˆå¤šä¸è¶³çš„åœ°æ–¹ï¼Œä¾‹å¦‚æœ¬æ¨¡å‹åœ¨æ•°æ®é›†æ–¹é¢çš„è¿˜æ²¡åšå¾ˆç²¾ç»†çš„è°ƒä¼˜ï¼Œè¿˜æœ‰æ—¶å€™æ ‡ç‚¹ç¬¦å·ä¼šé”™è¯¯ã€‚

æ¬¢è¿å¤§å®¶ä¸€èµ·è®¨è®ºï¼Œå¦‚æœå¤§å®¶æœ‰æ•°æ®é›†ï¼Œå¯ä»¥åœ¨ issue ç•™è¨€è®¨è®ºã€‚

## ğŸ’• è‡´è°¢

- [**xtuner**](https://github.com/InternLM/xtuner)

æ„Ÿè°¢ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ¨å‡ºçš„ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ï¼Œä¸ºæˆ‘ä»¬çš„é¡¹ç›®æä¾›å®è´µçš„æŠ€æœ¯æŒ‡å¯¼å’Œå¼ºå¤§çš„ç®—åŠ›æ”¯æŒã€‚

## å¼€æºè®¸å¯è¯

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache License 2.0 å¼€æºè®¸å¯è¯](https://github.com/PeterH0323/ancient-chat-llm/LICENSE) åŒæ—¶ï¼Œè¯·éµå®ˆæ‰€ä½¿ç”¨çš„æ¨¡å‹ä¸æ•°æ®é›†çš„è®¸å¯è¯ã€‚
